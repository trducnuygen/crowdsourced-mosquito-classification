{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Compare the performance of one-stage classification (only YOLO) vs two-stage classifcation (YOLO + CLIP) on test 4 class, no GBIF data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.16 (you have 1.4.11). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "# get test set\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from src.experiments import ExperimentMosquitoClassifier\n",
    "from src.classification import MosquitoClassifier\n",
    "# from openmax.openmax import OpenMaxYOLOCLIP\n",
    "import src.data_loader as dl\n",
    "\n",
    "from openmax.clf_utils import calculate_iou\n",
    "\n",
    "# IMG_SIZE = (299, 299) \n",
    "IMG_SIZE = (224, 224)\n",
    "USE_CHANNEL_LAST = False\n",
    "DATASET = \"laion\"\n",
    "DEVICE = \"cuda:0\"\n",
    "PRESERVE_ASPECT_RATIO = False\n",
    "SHIFT = 0\n",
    "\n",
    "# clip train has GBIF data.\n",
    "clip_model_path = './checkpoints/CLIP_anno2/epoch=6-val_loss=0.5844640731811523-val_f1_score=0.9127286076545715-val_multiclass_accuracy=0.9220854043960571.ckpt'\n",
    "\n",
    "# for openset\n",
    "CLASS_DICT = {\n",
    "    \"albopictus\":           th.tensor(0, dtype=th.float),\n",
    "    \"culex\":                th.tensor(1, dtype=th.float),\n",
    "    \"japonicus/koreicus\":   th.tensor(2, dtype=th.float),\n",
    "    \"culiseta\":             th.tensor(3, dtype=th.float),\n",
    "}\n",
    "\n",
    "class_dict = {\n",
    "    \"albopictus\":           th.tensor(0, dtype=th.float),\n",
    "    \"culex\":                th.tensor(1, dtype=th.float),\n",
    "    \"japonicus/koreicus\":   th.tensor(2, dtype=th.float),\n",
    "    \"culiseta\":             th.tensor(3, dtype=th.float),\n",
    "    \"mosquito\":             th.tensor(4, dtype=th.float)\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 0\n",
    "dataset = 'datacomp_xl_s13b_b90k'\n",
    "aug = 'hca'\n",
    "bs = 16\n",
    "# img_size = (299, 299) \n",
    "img_size = (224, 224)\n",
    "shift_box = False\n",
    "\n",
    "img_dir = \"\" \n",
    "\n",
    "val_annotations_csv = \"../data_round_2/mosAlert_new_annotation_2/val_annotation_2.csv\"\n",
    "train_annotations_csv = \"../data_round_2/mosAlert_new_annotation_2/train_annotation_2.csv\"\n",
    "test_annotations_csv = \"../data_round_2/mosAlert_new_annotation_2/test_annotation_2.csv\"\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(train_annotations_csv)\n",
    "\n",
    "train_df[\"img_fName\"] = img_dir + train_df[\"img_fName\"]\n",
    "\n",
    "val_df = pd.read_csv(val_annotations_csv)\n",
    "test_df = pd.read_csv(test_annotations_csv)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True) # shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2412, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_test = test_df[test_df[\"class_label\"] != \"mosquito\"]\n",
    "closed_test[\"class_label\"].value_counts()\n",
    "\n",
    "closed_test_dl = dl.TestYOLOCLIPDataset(annotations_df=closed_test, \n",
    "                                  class_dict=class_dict,\n",
    "                                  img_dir=img_dir,\n",
    "                                #   transform=dl.pre_process(dataset),\n",
    "                                #   data_augment=dl.aug(\"resize\", img_size),\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(img, yolo_model):\n",
    "    # detect mosquito\n",
    "    results = yolo_model(img, verbose=False, device=DEVICE, max_det=1)\n",
    "    img_w, img_h, _ = img.shape\n",
    "    bbox = [0, 0, img_w, img_h]\n",
    "    conf = 0.0\n",
    "    \n",
    "    for result in results:\n",
    "        _bbox = [0, 0, img_w, img_h]\n",
    "        # _label = \"albopictus\"\n",
    "        _conf = 0.0\n",
    "\n",
    "        bboxes_tmp = result.boxes.xyxy.tolist()\n",
    "        # labels_tmp = result.boxes.cls.tolist()\n",
    "        confs_tmp = result.boxes.conf.tolist()\n",
    "\n",
    "        for bbox_tmp, conf_tmp in zip(bboxes_tmp, confs_tmp):\n",
    "            if conf_tmp > _conf:\n",
    "                _bbox = bbox_tmp\n",
    "                _conf = conf_tmp\n",
    "\n",
    "        if _conf > conf:\n",
    "            bbox = _bbox\n",
    "            # label = _label\n",
    "            conf = _conf\n",
    "\n",
    "    bbox = [int(float(mcb)) for mcb in bbox]\n",
    "\n",
    "    return bbox, conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch yolo\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize model\n",
    "yolo_path = \"/home/pc2/Downloads/AI-Crowd/Mosquito-Classifiction/experiments/yolo/runs/detect/train_4class_noGbif/weights/best.pt\"\n",
    "yolo_predictor = YOLO(yolo_path, task='predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openmax.clf_utils import *\n",
    "\n",
    "class YOLOCLip:\n",
    "    def __init__(self, yolo_path, clip_path):\n",
    "\n",
    "        self.yolo = YOLO(yolo_path, task='detect')\n",
    "        self.clip= MosquitoClassifier.load_from_checkpoint(clip_path, \n",
    "                                                                      head_version=7, \n",
    "                                                                      map_location=th.device(DEVICE)).eval()\n",
    "    def predict(self, img):\n",
    "        x = prepCLIP2(img, self.yolo).to(DEVICE)\n",
    "        with th.no_grad():\n",
    "            logits = self.clip(x)\n",
    "        return torch.argmax(logits, dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded ViT-L-14 model config.\n",
      "INFO:root:Loading pretrained ViT-L-14 weights (datacomp_xl_s13b_b90k).\n"
     ]
    }
   ],
   "source": [
    "yolo_clip = YOLOCLip(yolo_path, clip_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_yolo(img, yolo_model):\n",
    "    results = yolo_model(img, verbose=False, device=DEVICE, max_det=1)\n",
    "    conf = 0.0\n",
    "    label = -1\n",
    "    \n",
    "    for result in results:       \n",
    "        _conf = 0.0\n",
    "\n",
    "        labels_tmp = result.boxes.cls.tolist()\n",
    "        # print(labels_tmp)\n",
    "\n",
    "        if len(labels_tmp) == 0:\n",
    "            _label = -1\n",
    "        else:\n",
    "            _label = labels_tmp[0]\n",
    "        confs_tmp = result.boxes.conf.tolist()\n",
    "\n",
    "        for label_tmp, conf_tmp in zip(labels_tmp, confs_tmp):\n",
    "            if conf_tmp > _conf:\n",
    "                \n",
    "                _label = label_tmp\n",
    "                _conf = conf_tmp\n",
    "\n",
    "        if _conf > conf:           \n",
    "            label = _label\n",
    "            conf = _conf\n",
    "\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues = []\n",
    "preds = []\n",
    "empty_count = 0\n",
    "labels = list(range(4))\n",
    "for batch in closed_test_dl:\n",
    "    img = batch[0]\n",
    "    y_true = batch[1]\n",
    "    pred = process_yolo(img, yolo_predictor)\n",
    "\n",
    "    if pred == -1:\n",
    "        # random a label different from y_true\n",
    "        pred = np.random.choice([i for i in labels if i != y_true])\n",
    "        empty_count += 1\n",
    "\n",
    "    # print(x.shape)\n",
    "    # pred = cls(x)\n",
    "    \n",
    "    # print(calculate_iou(bbox, bbox_true))\n",
    "    # print(torch.argmax(pred, dim=1))\n",
    "    # print(y_true)\n",
    "\n",
    "    # print(calculate_iou(bbox, bbox_true))\n",
    "    # pred is now a numpy array\n",
    "    preds.append(pred)\n",
    "    trues.append(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.92      0.69       516\n",
      "         1.0       0.92      0.39      0.54       634\n",
      "         2.0       0.22      0.52      0.31        54\n",
      "         3.0       0.39      0.23      0.29        96\n",
      "\n",
      "    accuracy                           0.59      1300\n",
      "   macro avg       0.52      0.51      0.46      1300\n",
      "weighted avg       0.71      0.59      0.58      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy, precision, recall, macro f1\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(trues, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.23%\n",
      "F1 Score for class '0.0': 69.44%\n",
      "F1 Score for class '1.0': 54.38%\n",
      "F1 Score for class '2.0': 31.28%\n",
      "F1 Score for class '3.0': 28.95%\n",
      "F1 Score for class 'macro avg': 46.02%\n",
      "F1 Score for class 'weighted avg': 57.52%\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'trues' contains true labels and 'preds' contains predicted labels\n",
    "report = classification_report(trues, preds, output_dict=True)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(trues, preds)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print F1 score for each class in percentage format\n",
    "for class_label, metrics in report.items():\n",
    "    if class_label != 'accuracy':\n",
    "        f1_score_percentage = metrics['f1-score'] * 100\n",
    "        print(f\"F1 Score for class '{class_label}': {f1_score_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5923076923076923\n",
      "F1:  0.4601518670616525\n",
      "Precision:  0.5229929690502566\n",
      "Recall:  0.5136657880185959\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(trues, preds))\n",
    "print(\"F1: \", f1_score(trues, preds, average='macro'))\n",
    "print(\"Precision: \", precision_score(trues, preds, average='macro'))\n",
    "print(\"Recall: \", recall_score(trues, preds, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next for yolo clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues = []\n",
    "preds = []\n",
    "ious = []\n",
    "for batch in closed_test_dl:\n",
    "    img = batch[0]\n",
    "    y_true = batch[1]\n",
    "    bbox_true = batch[2]\n",
    "\n",
    "    bbox, conf = get_bbox(img, yolo_clip.yolo)\n",
    "\n",
    "    pred = yolo_clip.predict(img)\n",
    "\n",
    "    # print(x.shape)\n",
    "    # pred = cls(x)\n",
    "    \n",
    "    # print(calculate_iou(bbox, bbox_true))\n",
    "    # print(torch.argmax(pred, dim=1))\n",
    "    # print(y_true)\n",
    "\n",
    "    # print(calculate_iou(bbox, bbox_true))\n",
    "    # pred is now a numpy array\n",
    "    preds.append(pred)\n",
    "    trues.append(y_true)\n",
    "    ious.append(calculate_iou(bbox, bbox_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(trues, preds))\n",
    "print(\"F1: \", f1_score(trues, preds, average='macro'))\n",
    "print(\"Precision: \", precision_score(trues, preds, average='macro'))\n",
    "print(\"Recall: \", recall_score(trues, preds, average='macro'))\n",
    "print(\"mIoU: \", np.mean(ious))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
