{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# GradCAM\n",
    "from pytorch_grad_cam import (\n",
    "    GradCAM,\n",
    "    ScoreCAM,\n",
    "    GradCAMPlusPlus,\n",
    "    AblationCAM,\n",
    "    XGradCAM,\n",
    "    EigenCAM,\n",
    "    EigenGradCAM,\n",
    "    LayerCAM,\n",
    "    FullGrad,\n",
    ")\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from pytorch_grad_cam.ablation_layer import AblationLayerVit\n",
    "\n",
    "# Change the working directory to 'experiments'\n",
    "current_dir = os.getcwd()\n",
    "folder2_path = os.path.abspath(os.path.join(current_dir, '..', 'experiments'))\n",
    "sys.path.append(folder2_path)\n",
    "\n",
    "# Module\n",
    "import src.classification as lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_layers(model):\n",
    "    layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if name == '':\n",
    "            continue\n",
    "        layers.append(module)\n",
    "    return layers\n",
    "\n",
    "def reshape_transform(tensor, height=16, width=16):\n",
    "    result = tensor[:, 1:, :].reshape(tensor.size(0), height, width, tensor.size(2))\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result\n",
    "\n",
    "def cropImg(df, imagePath):\n",
    "    image = Image.open(imagePath)\n",
    "    \n",
    "    # Extract bbox\n",
    "    bbx_coords = df[df.img_fName == imagePath].iloc[0][['bbx_xtl', 'bbx_ytl', 'bbx_xbr', 'bbx_ybr']]\n",
    "    bbx_xtl, bbx_ytl, bbx_xbr, bbx_ybr = bbx_coords\n",
    "\n",
    "    # Crop the image\n",
    "    return image.crop((bbx_xtl, bbx_ytl, bbx_xbr, bbx_ybr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    image_list = []\n",
    "    aug_smooth = False\n",
    "    eigen_smooth = False\n",
    "    method = 'gradcam'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "methods = {\n",
    "    \"gradcam\": GradCAM,\n",
    "    \"scorecam\": ScoreCAM,\n",
    "    \"gradcam++\": GradCAMPlusPlus,\n",
    "    \"ablationcam\": AblationCAM,\n",
    "    \"xgradcam\": XGradCAM,\n",
    "    \"eigencam\": EigenCAM,\n",
    "    \"eigengradcam\": EigenGradCAM,\n",
    "    \"layercam\": LayerCAM,\n",
    "    \"fullgrad\": FullGrad\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_path = '../data_round_2/mosAlert_new_annotation_2/test_annotation_2.csv'\n",
    "df = pd.read_csv(anno_path).sort_values(by='img_fName').reset_index(drop=True)\n",
    "\n",
    "checkpoint_path = './modelCheckpoint/epoch=6-val_loss=0.5844640731811523-val_f1_score=0.9127286076545715-val_multiclass_accuracy=0.9220854043960571.ckpt'\n",
    "modelCLIP = lc.MosquitoClassifier.load_from_checkpoint(checkpoint_path, map_location=torch.device('cuda'))\n",
    "target_layers = get_all_layers(modelCLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing method\n",
    "args.image_list = df.img_fName.tolist()\n",
    "args.method = 'gradcam'\n",
    "\n",
    "if args.method not in list(methods.keys()):\n",
    "    raise Exception(f\"method should be one of {list(methods.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in args.image_list:\n",
    "\n",
    "    imgName = image_path.split('/')[-1].split('.')[0]\n",
    "    label = df[df.img_fName == image_path].class_label.values[0]\n",
    "    \n",
    "    # Read the image\n",
    "    newImg = cropImg(df, image_path).convert('RGB')\n",
    "    rgb_img = np.array(newImg)\n",
    "    rgb_img = cv2.resize(rgb_img, (224, 224))\n",
    "    rgb_img = np.float32(rgb_img) / 255\n",
    "    input_tensor = preprocess_image(rgb_img, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "\n",
    "    if args.method == \"ablationcam\":\n",
    "        cam = methods[args.method](model=modelCLIP, target_layers=target_layers, reshape_transform=reshape_transform, ablation_layer=AblationLayerVit())\n",
    "    else:\n",
    "        cam = methods[args.method](model=modelCLIP, target_layers=target_layers, reshape_transform=reshape_transform)\n",
    "    \n",
    "    # CAM\n",
    "    targets = None\n",
    "    cam.batch_size = 32\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets, eigen_smooth=args.eigen_smooth, aug_smooth=args.aug_smooth)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    # Visualize CAM\n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam)\n",
    "\n",
    "\n",
    "    # Output\n",
    "    outputPath = 'CLIPgradcamResult'\n",
    "    outputClass = str(label)\n",
    "    outputImgName = f'{imgName}_{args.method}_cam.jpg'\n",
    "    full_output_path = os.path.join(outputPath, outputClass)\n",
    "\n",
    "    os.makedirs(full_output_path, exist_ok=True)\n",
    "    full_output_path = os.path.join(full_output_path, outputImgName)\n",
    "\n",
    "    cv2.imwrite(full_output_path, cam_image)\n",
    "    print(imgName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
